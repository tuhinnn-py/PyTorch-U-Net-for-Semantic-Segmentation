{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import load_img,img_to_array\nimport torch\nimport os\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision.transforms import transforms\nclass SemanticDataset(Dataset):\n    def __init__(self,root_dir,image_inp_size,seg_img_inp_size,transforms=None):\n        self.root_dir=root_dir\n        self.image_inp_size=image_inp_size\n        self.seg_img_inp_size=seg_img_inp_size\n        self.transforms=transforms\n        \n    def __len__(self):\n        return len(os.listdir(os.path.join(self.root_dir,'SegmentationClass')))\n    \n    def __getitem__(self,idx):\n        if torch.is_tensor(idx):\n            idx=idx.tolist()\n\n        seg_img_name=os.path.join(self.root_dir,'SegmentationClass',os.listdir(os.path.join(self.root_dir,'SegmentationClass'))[idx])\n        img_name=seg_img_name.replace('SegmentationClass','JPEGImages')[:-4]+'.jpg'\n        image=img_to_array(load_img(img_name,target_size=self.image_inp_size))/255\n        seg_img=img_to_array(load_img(seg_img_name,target_size=self.seg_img_inp_size))\n        sample={'image': image, 'seg_image': seg_img}\n        \n        if self.transforms:\n            sample=self.transforms(sample)\n            \n        return sample\n    \nclass ToTensor(object):\n    def __call__(self, sample):\n        image, seg_img = sample['image'], sample['seg_image']\n        image=np.transpose(image,(2,0,1))\n        return {'image': torch.from_numpy(image).cuda(),'seg_image': torch.tensor(voc_label_indices(seg_img,build_colormap2label()),dtype=torch.long).cuda()}\n    \ntransforms=transforms.Compose([ToTensor()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Not in chronological order. Needs to be run first!\nVOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n                [0, 64, 128]]\nVOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n               'diningtable', 'dog', 'horse', 'motorbike', 'person',\n               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']\n\ndef build_colormap2label():\n    colormap2label=np.zeros(256**3)\n    for i,color in enumerate(VOC_COLORMAP):\n        idx=(color[0]*256+color[1])*256+color[2]\n        colormap2label[idx]=i\n    return colormap2label\n\ndef voc_label_indices(colormap,colormap2label):\n    colormap=np.asarray(colormap,dtype=np.int32)\n    idx=(colormap[:,:,0]*256+colormap[:,:,1])*256+colormap[:,:,2]\n    return colormap2label[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nclass Network(nn.Module):\n    def __init__(self,n_categories):\n        super(Network, self).__init__()\n        \n        self.conv1=nn.Conv2d(in_channels=3,out_channels=64,kernel_size=3)\n        self.batchn1=nn.BatchNorm2d(num_features=64)\n        self.relu1=nn.ReLU(inplace=True)\n        \n        self.conv2=nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3)\n        self.batchn2=nn.BatchNorm2d(num_features=64)\n        self.relu2=nn.ReLU(inplace=True)\n        \n        self.down1=nn.Conv2d(in_channels=64,out_channels=1,kernel_size=2,stride=2)\n        '''\n        self.conv3=nn.Conv2d(in_channels=1,out_channels=128,kernel_size=3)\n        self.batchn3=nn.BatchNorm2d(num_features=128)\n        self.relu3=nn.ReLU(inplace=True)\n        \n        self.conv4=nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3)\n        self.batchn4=nn.BatchNorm2d(num_features=128)\n        self.relu4=nn.ReLU(inplace=True)\n        \n        self.down2=nn.Conv2d(in_channels=128,out_channels=1,kernel_size=2,stride=2)\n        '''\n        self.conv5=nn.Conv2d(in_channels=1,out_channels=256,kernel_size=3)\n        self.batchn5=nn.BatchNorm2d(num_features=256)\n        self.relu5=nn.ReLU(inplace=True)\n        \n        self.conv6=nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3)\n        self.batchn6=nn.BatchNorm2d(num_features=256)\n        self.relu6=nn.ReLU(inplace=True)\n        \n        self.down3=nn.Conv2d(in_channels=256,out_channels=1,kernel_size=2,stride=2)\n        '''\n        self.conv7=nn.Conv2d(in_channels=1,out_channels=512,kernel_size=3)\n        self.batchn7=nn.BatchNorm2d(num_features=512)\n        self.relu7=nn.ReLU(inplace=True)\n        \n        self.conv8=nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3)\n        self.batchn8=nn.BatchNorm2d(num_features=512)\n        self.relu8=nn.ReLU(inplace=True)\n        \n        self.down4=nn.Conv2d(in_channels=512,out_channels=1,kernel_size=2,stride=2)\n        '''\n        self.conv9=nn.Conv2d(in_channels=1,out_channels=1024,kernel_size=3)\n        self.batchn9=nn.BatchNorm2d(num_features=1024)\n        self.relu9=nn.ReLU(inplace=True)\n        \n        self.conv10=nn.Conv2d(in_channels=1024,out_channels=1024,kernel_size=3)\n        self.batchn10=nn.BatchNorm2d(num_features=1024)\n        self.relu10=nn.ReLU(inplace=True)\n        '''\n        self.up1=nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=2,stride=2)\n        \n        self.conv11=nn.Conv2d(in_channels=1024,out_channels=512,kernel_size=3)\n        self.batchn11=nn.BatchNorm2d(num_features=512)\n        self.relu11=nn.ReLU(inplace=True)\n        \n        self.conv12=nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3)\n        self.batchn12=nn.BatchNorm2d(num_features=512)\n        self.relu12=nn.ReLU(inplace=True)\n        '''\n        self.up2=nn.ConvTranspose2d(in_channels=1024,out_channels=256,kernel_size=2,stride=2)#1024,512\n        \n        self.conv13=nn.Conv2d(in_channels=512,out_channels=256,kernel_size=3)\n        self.batchn13=nn.BatchNorm2d(num_features=256)\n        self.relu13=nn.ReLU(inplace=True)\n        \n        self.conv14=nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3)\n        self.batchn14=nn.BatchNorm2d(num_features=256)\n        self.relu14=nn.ReLU(inplace=True)\n        '''\n        self.up3=nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=2,stride=2)\n        \n        self.conv15=nn.Conv2d(in_channels=256,out_channels=128,kernel_size=3)\n        self.batchn15=nn.BatchNorm2d(num_features=128)\n        self.relu15=nn.ReLU(inplace=True)\n        \n        self.conv16=nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3)\n        self.batchn16=nn.BatchNorm2d(num_features=128)\n        self.relu16=nn.ReLU(inplace=True)\n        '''\n        self.up4=nn.ConvTranspose2d(in_channels=256,out_channels=64,kernel_size=2,stride=2)#256,128\n        \n        self.conv17=nn.Conv2d(in_channels=128,out_channels=64,kernel_size=3)\n        self.batchn17=nn.BatchNorm2d(num_features=64)\n        self.relu17=nn.ReLU(inplace=True)\n        \n        self.conv18=nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3)\n        self.batchn18=nn.BatchNorm2d(num_features=64)\n        self.relu18=nn.ReLU(inplace=True)\n        \n        self.out=nn.Conv2d(in_channels=64,out_channels=n_categories,kernel_size=1)\n        \n    def forward(self,t):\n        \n        t=self.conv1(t)\n        t=self.batchn1(t)\n        t=self.relu1(t)\n        \n        t=self.conv2(t)\n        t=self.batchn2(t)\n        c1=self.relu2(t)\n        \n        t=self.down1(c1)\n        '''\n        t=self.conv3(t)\n        t=self.batchn3(t)\n        t=self.relu3(t)\n        \n        t=self.conv4(t)\n        t=self.batchn4(t)\n        c2=self.relu4(t)\n        \n        t=self.down2(c2)\n        '''\n        t=self.conv5(t)\n        t=self.batchn5(t)\n        t=self.relu5(t)\n        \n        t=self.conv6(t)\n        t=self.batchn6(t)\n        c3=self.relu6(t)\n        \n        t=self.down3(c3)\n        '''\n        t=self.conv7(t)\n        t=self.batchn7(t)\n        t=self.relu7(t)\n        \n        t=self.conv8(t)\n        t=self.batchn8(t)\n        c4=self.relu8(t)\n        \n        t=self.down4(c4)\n        '''\n        t=self.conv9(t)\n        t=self.batchn9(t)\n        t=self.relu9(t)\n        \n        t=self.conv10(t)\n        t=self.batchn10(t)\n        t=self.relu10(t)\n        '''\n        t=self.up1(t)\n        c4=c4[:,:,4:20,4:20]\n        t=torch.cat((t,c4),dim=1)\n        \n        t=self.conv11(t)\n        t=self.batchn11(t)\n        t=self.relu11(t)\n        \n        t=self.conv12(t)\n        t=self.batchn12(t)\n        t=self.relu12(t)\n        '''\n        t=self.up2(t)\n        \n        c3=c3[:,:,4:118,4:118]  #40,118  16,4\n        t=torch.cat((t,c3),dim=1)\n        \n        t=self.conv13(t)\n        t=self.batchn13(t)\n        t=self.relu13(t)\n        \n        t=self.conv14(t)\n        t=self.batchn14(t)\n        t=self.relu14(t)\n        '''\n        t=self.up3(t)\n        c2=c2[:,:,41:81,41:81]\n        t=torch.cat((t,c2),dim=1)\n        \n        t=self.conv15(t)\n        t=self.batchn15(t)\n        t=self.relu15(t)\n        \n        t=self.conv16(t)\n        t=self.batchn16(t)\n        t=self.relu16(t)\n        '''\n        t=self.up4(t)\n        \n        c1=c1[:,:,16:236,16:236]  #16,90  162,236\n        t=torch.cat((t,c1),dim=1)\n        \n        t=self.conv17(t)\n        t=self.batchn17(t)\n        t=self.relu17(t)\n        \n        t=self.conv18(t)\n        t=self.batchn18(t)\n        t=self.relu18(t)\n        \n        t=self.out(t)\n        return t\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import optim\ndevice = torch.device(\"cuda\")\nseg_model=Network(len(VOC_CLASSES))\nseg_model.load_state_dict(torch.load('/kaggle/input/sem-seg-model/Segnet_pytorch.pkl',map_location=\"cuda:0\"),strict=False)\nseg_model.to(device)\nseg_model.eval()\nEPOCHS=200\nBATCH_SIZE=32\nLEARNING_RATE=0.0001\noptimizer=optim.Adam(seg_model.parameters(), lr=LEARNING_RATE)\n#....scheduler=optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,70,90], gamma=0.1, last_epoch=-1)\nseg_dataset = SemanticDataset(root_dir='/kaggle/input/pascalvoc2009/VOCdevkit/VOC2009',image_inp_size=(256,256,3),seg_img_inp_size=(216,216,3),transforms=transforms)\nseg_dataloader=DataLoader(seg_dataset,batch_size=BATCH_SIZE,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(EPOCHS):\n    epoch_loss=0\n    for i,sample in enumerate(seg_dataloader):\n        X,y=sample['image'],sample['seg_image']\n        y_pred=seg_model(X)\n        optimizer.zero_grad()\n        loss=F.cross_entropy(y_pred,y)\n        epoch_loss+=loss.item()/BATCH_SIZE\n        loss.backward()\n        optimizer.step()\n    #scheduler.step()\n    \n    print('Epoch: {0}, Loss: {1}'.format(epoch,epoch_loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(seg_model.state_dict(),'/kaggle/working/A.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torchvision\ndef get_seg_img(seg_model,n_samples):\n    \n    dataloader=DataLoader(seg_dataset,batch_size=n_samples,shuffle=True)\n    samples=iter(dataloader)\n    sample=next(samples)\n    X,y=sample['image'],sample['seg_image'].tolist()\n    y_pred=torch.argmax(nn.Softmax(dim=1)(seg_model(X)),dim=1).tolist()\n    #print(torch.argmax(nn.Softmax(dim=1)(seg_model(X)),dim=1).sum())\n    #'''\n    for i in range(n_samples):\n        for j in range(216):\n            for k in range(216):\n                y_pred[i][j][k]=VOC_COLORMAP[y_pred[i][j][k]]\n                y[i][j][k]=VOC_COLORMAP[y[i][j][k]]\n    X=X.cpu()\n    plt.figure(figsize=(15,15))\n    for i in range(n_samples):\n        plt.subplot(1,n_samples,i+1)\n        plt.imshow(np.transpose(X[i],(1,2,0)))\n    plt.show()\n    \n    plt.figure(figsize=(15,15))\n    for i in range(n_samples):\n        plt.subplot(1,n_samples,i+1)\n        plt.imshow(y[i])\n    plt.show()\n    \n    return np.array(y_pred)\n    #'''\n    \nsamples=get_seg_img(seg_model,4)\n#'''\nplt.figure(figsize=(15,15))\nfor i in range(samples.shape[0]):\n    plt.subplot(1,samples.shape[0],i+1)\n    plt.imshow(samples[i])\nplt.show()\n#'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary\nseg_model=Network(len(VOC_CLASSES)).cuda()\nsummary(seg_model,(3,256,256))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}